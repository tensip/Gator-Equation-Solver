{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author @Sippapas Sukpholtham\n",
    "Master's Student in Mechanical and Aerospace Engineering\n",
    "University of Florida\n",
    "Dec, 2022\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# from CNN.CNN_2 import CNN\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13600, 785)\n",
      "(13600, 784)\n",
      "(13600,)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "raw_data = np.load('data_for_training.npy')\n",
    "print(raw_data.shape)\n",
    "\n",
    "data = raw_data[:,:-1]\n",
    "print(data.shape)\n",
    "\n",
    "labels = raw_data[:,-1]\n",
    "print(labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rearranging the data so that we can input the data into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the dimension before inputting into Neural Network\n",
    "data = data.reshape(data.shape[0], 28, 28)\n",
    "data = np.array(data, dtype='int64')\n",
    "labels = np.array(labels, dtype='int64')\n",
    "\n",
    "# Convert type of data from numpy to tensor\n",
    "data = torch.from_numpy(data)\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the example of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# test = np.array(x_train[0])\n",
    "N = random.randint(1, 10000)\n",
    "test = data[N].reshape(28, 28, 1)\n",
    "plt.imshow(test, cmap='gray')\n",
    "print(labels[N])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the data from numpy to tensor and reshaping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13600, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization of test data: shape from (number_of_data, 150, 150) to (number_of_data, 1, 150, 150)\n",
    "data_test = torch.unsqueeze(data, dim=1).type(torch.FloatTensor)/255\n",
    "data_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks that was designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=32,            # n_filters\n",
    "                kernel_size=3,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # padding\n",
    "            ),                              \n",
    "            nn.BatchNorm2d(32),             # normalization the data\n",
    "            nn.ReLU(),                      # activation function\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area\n",
    "            nn.Dropout2d(p=0.3)             # drop out 10% of data\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(32, 64, 3, 1, 2),     \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 128, 3, 1, 2),    \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(         \n",
    "            nn.Conv2d(128, 128, 3, 1, 2),     \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        # self.conv5 = nn.Sequential(         \n",
    "        #     nn.Conv2d(64, 64, 3, 1, 2),     \n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),                      \n",
    "        #     nn.MaxPool2d(2),                \n",
    "        #     nn.Dropout2d(p=0.1),\n",
    "        # )\n",
    "\n",
    "        self.out1 = nn.Sequential(nn.Linear(1152, 1000), \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p=0.3))\n",
    "        self.out2 = nn.Sequential(nn.Linear(1000 , 500),  \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p=0.3))\n",
    "        self.out3 = nn.Sequential(nn.Linear(500 , 17))  # last fully connected layer,  output 17 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        # x = self.conv5(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the output of conv2 to (BATCH_SIZE, 64 * 3 * 3)\n",
    "        # Linear layers          \n",
    "        output = self.out1(x)\n",
    "        output = self.out2(output)\n",
    "        output = self.out3(output)\n",
    "        return output, x\n",
    "\n",
    "# Call the model\n",
    "cnn = CNN().to(device)\n",
    "\n",
    "# Download the weights from training\n",
    "cnn.load_state_dict(torch.load('model1.pth'), strict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the data by using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.99603\n",
      "Confusion matrix of test set : \n",
      " [[797   0   0   0   0   0   1   0   0   0   0   0   0   0   1   0   1]\n",
      " [  0 778   1   1   0   0   0  16   0   0   0   0   0   1   0   3   0]\n",
      " [  0   0 797   0   1   0   0   0   0   0   1   0   1   0   0   0   0]\n",
      " [  0   0   0 799   0   0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 799   0   0   0   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 798   1   0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0 793   0   0   0   0   0   0   0   6   0   0]\n",
      " [  0   0   0   0   0   0   0 798   0   0   0   1   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 799   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   1   0   0   0 799   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   1   0   0   0   0   0 797   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 800   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 800   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0   0   0   0   0 799   0   0   0]\n",
      " [  0   0   0   0   0   0   5   0   0   0   0   0   0   0 795   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 800   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   1   0   0   0   0   0 798]]\n"
     ]
    }
   ],
   "source": [
    "# Testing data\n",
    "cnn.eval()\n",
    "\n",
    "pred_y = np.array([])\n",
    "for step in range(1, int((len(data_test))+1)):\n",
    "    test_output = cnn(data_test[(step-1):step].to(device))[0]\n",
    "    _ , prediction = torch.max(test_output, 1)\n",
    "    pred_y = np.append(pred_y, prediction.cpu().numpy())\n",
    "    data_test[(step-1):step].to('cpu')\n",
    "\n",
    "accuracy = accuracy_score(labels, pred_y)\n",
    "cm = confusion_matrix(labels, pred_y)\n",
    "\n",
    "print('Test accuracy: %.5f' % accuracy)\n",
    "print(\"Confusion matrix of test set : \\n\",cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37e539c8d40a03e9510857e177c5bc496c3bcb5968daf996e906447222e565b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
